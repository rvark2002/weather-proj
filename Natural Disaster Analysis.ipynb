{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac8585e",
   "metadata": {},
   "source": [
    "# Analysis of Natural Disasters in US over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc8aec",
   "metadata": {},
   "source": [
    "## Ritvik Varkhedkar and Thomas Klaus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2c9c8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f9ac4",
   "metadata": {},
   "source": [
    "Natural Disasters, A powerful, uncontrollable force of nature that causes damage and casualties wherever it occurs. Natural Disasters come in many forms - from hurricanes to ice storms, fires, and earthquakes. With the damage and destruction caused by Natural Disasters, it is essential to analyze and find trends in data to prevent damage caused by future storms.\n",
    "In this project:\n",
    "1) we aim to analyze a data set of Natural Disasters (obtained from https://www.kaggle.com/headsortails/us-natural-disaster-declarations )\n",
    "\n",
    "2) Look for patterns or trends in many visual forms - graphs, maps\n",
    "\n",
    "3) Look at potential correlating factors\n",
    "\n",
    "4) provide a replicable tutorial so that one can come up with their conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72f90d",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "Here we are going to import some libraries that will be important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c297a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests #Used to grab html code from websites\n",
    "import pandas as pd #Used to create Dataframes, tables, and etc\n",
    "import numpy as np #Used to do some calculations\n",
    "import matplotlib.pyplot as plt # Used to create plots\n",
    "import random #Used to generate a random number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459bb56",
   "metadata": {},
   "source": [
    "## Initial Data set analysis\n",
    "\n",
    "It is important to understand what data we are reading, below is the head of the data set that we will be using for our analysis of Natural Disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b053ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fema_declaration_string</th>\n",
       "      <th>disaster_number</th>\n",
       "      <th>state</th>\n",
       "      <th>declaration_type</th>\n",
       "      <th>declaration_date</th>\n",
       "      <th>fy_declared</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>declaration_title</th>\n",
       "      <th>ih_program_declared</th>\n",
       "      <th>ia_program_declared</th>\n",
       "      <th>...</th>\n",
       "      <th>incident_begin_date</th>\n",
       "      <th>incident_end_date</th>\n",
       "      <th>disaster_closeout_date</th>\n",
       "      <th>fips</th>\n",
       "      <th>place_code</th>\n",
       "      <th>designated_area</th>\n",
       "      <th>declaration_request_number</th>\n",
       "      <th>hash</th>\n",
       "      <th>last_refresh</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR-1-GA</td>\n",
       "      <td>1</td>\n",
       "      <td>GA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1954-06-01T00:00:00Z</td>\n",
       "      <td>13000</td>\n",
       "      <td>0</td>\n",
       "      <td>Statewide</td>\n",
       "      <td>53013</td>\n",
       "      <td>bb121323c9c29d3bef0c9a3f134bfd8b5ecff148</td>\n",
       "      <td>2021-07-13T23:01:19Z</td>\n",
       "      <td>60c3b7a9a0ee349d71025780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR-2-TX</td>\n",
       "      <td>2</td>\n",
       "      <td>TX</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado &amp; Heavy Rainfall</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1958-01-01T00:00:00Z</td>\n",
       "      <td>48000</td>\n",
       "      <td>0</td>\n",
       "      <td>Statewide</td>\n",
       "      <td>53003</td>\n",
       "      <td>c879557e78d059e6847e7688388c339d10f51979</td>\n",
       "      <td>2021-07-13T23:01:19Z</td>\n",
       "      <td>60c3b7a9a0ee349d71025783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR-3-LA</td>\n",
       "      <td>3</td>\n",
       "      <td>LA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1960-02-01T00:00:00Z</td>\n",
       "      <td>22000</td>\n",
       "      <td>0</td>\n",
       "      <td>Statewide</td>\n",
       "      <td>53005</td>\n",
       "      <td>4fb19699fdbba1387ffa2263fcc4a4e37a1de6d6</td>\n",
       "      <td>2021-07-13T23:01:19Z</td>\n",
       "      <td>60c3b7a9a0ee349d71025777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR-4-MI</td>\n",
       "      <td>4</td>\n",
       "      <td>MI</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1956-02-01T00:00:00Z</td>\n",
       "      <td>26000</td>\n",
       "      <td>0</td>\n",
       "      <td>Statewide</td>\n",
       "      <td>53004</td>\n",
       "      <td>87a0c1dd5da249767f545e0c0a43f917e4e9ca83</td>\n",
       "      <td>2021-07-13T23:01:19Z</td>\n",
       "      <td>60c3b7a9a0ee349d7102577a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR-5-MT</td>\n",
       "      <td>5</td>\n",
       "      <td>MT</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Floods</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1955-12-01T00:00:00Z</td>\n",
       "      <td>30000</td>\n",
       "      <td>0</td>\n",
       "      <td>Statewide</td>\n",
       "      <td>53006</td>\n",
       "      <td>954449c15634fb45c8bea3ac975782793ccde050</td>\n",
       "      <td>2021-07-13T23:01:19Z</td>\n",
       "      <td>60c3b7a9a0ee349d71025774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fema_declaration_string  disaster_number state declaration_type  \\\n",
       "0                 DR-1-GA                1    GA               DR   \n",
       "1                 DR-2-TX                2    TX               DR   \n",
       "2                 DR-3-LA                3    LA               DR   \n",
       "3                 DR-4-MI                4    MI               DR   \n",
       "4                 DR-5-MT                5    MT               DR   \n",
       "\n",
       "       declaration_date  fy_declared incident_type         declaration_title  \\\n",
       "0  1953-05-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "1  1953-05-15T00:00:00Z         1953       Tornado  Tornado & Heavy Rainfall   \n",
       "2  1953-05-29T00:00:00Z         1953         Flood                     Flood   \n",
       "3  1953-06-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "4  1953-06-06T00:00:00Z         1953         Flood                    Floods   \n",
       "\n",
       "   ih_program_declared  ia_program_declared  ...   incident_begin_date  \\\n",
       "0                    0                    1  ...  1953-05-02T00:00:00Z   \n",
       "1                    0                    1  ...  1953-05-15T00:00:00Z   \n",
       "2                    0                    1  ...  1953-05-29T00:00:00Z   \n",
       "3                    0                    1  ...  1953-06-02T00:00:00Z   \n",
       "4                    0                    1  ...  1953-06-06T00:00:00Z   \n",
       "\n",
       "      incident_end_date disaster_closeout_date   fips place_code  \\\n",
       "0  1953-05-02T00:00:00Z   1954-06-01T00:00:00Z  13000          0   \n",
       "1  1953-05-15T00:00:00Z   1958-01-01T00:00:00Z  48000          0   \n",
       "2  1953-05-29T00:00:00Z   1960-02-01T00:00:00Z  22000          0   \n",
       "3  1953-06-02T00:00:00Z   1956-02-01T00:00:00Z  26000          0   \n",
       "4  1953-06-06T00:00:00Z   1955-12-01T00:00:00Z  30000          0   \n",
       "\n",
       "   designated_area  declaration_request_number  \\\n",
       "0        Statewide                       53013   \n",
       "1        Statewide                       53003   \n",
       "2        Statewide                       53005   \n",
       "3        Statewide                       53004   \n",
       "4        Statewide                       53006   \n",
       "\n",
       "                                       hash          last_refresh  \\\n",
       "0  bb121323c9c29d3bef0c9a3f134bfd8b5ecff148  2021-07-13T23:01:19Z   \n",
       "1  c879557e78d059e6847e7688388c339d10f51979  2021-07-13T23:01:19Z   \n",
       "2  4fb19699fdbba1387ffa2263fcc4a4e37a1de6d6  2021-07-13T23:01:19Z   \n",
       "3  87a0c1dd5da249767f545e0c0a43f917e4e9ca83  2021-07-13T23:01:19Z   \n",
       "4  954449c15634fb45c8bea3ac975782793ccde050  2021-07-13T23:01:19Z   \n",
       "\n",
       "                         id  \n",
       "0  60c3b7a9a0ee349d71025780  \n",
       "1  60c3b7a9a0ee349d71025783  \n",
       "2  60c3b7a9a0ee349d71025777  \n",
       "3  60c3b7a9a0ee349d7102577a  \n",
       "4  60c3b7a9a0ee349d71025774  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"us_disaster_declarations.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b3941",
   "metadata": {},
   "source": [
    "Looking at this table, we can notice a bunch of categories, the most important ones to us are:\n",
    "\n",
    "1) fema_declaration_string - This identifies what the disaster was. Sort of like a name\n",
    "\n",
    "2) state - tells us the state it was issued in\n",
    "\n",
    "3) fy_declared - tells us the year it was declared in\n",
    "\n",
    "4) incident_type - tells us what the disaster was\n",
    "\n",
    "The rest of the information is not really important to us, so we will remove that from our data set to during\n",
    "our cleaning process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19150ce6",
   "metadata": {},
   "source": [
    "## Initial Cleaning\n",
    "Here we are cleaning up the data set. What we are going to do is remove some Natural Disasters that we feel are not climate related,\n",
    "\n",
    "Ex: Terrorism, Chemical. \n",
    "\n",
    "Another thing we are going to do is remove some columns that we are not going to use. This way we can run code much more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d0df5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fema_declaration_string</th>\n",
       "      <th>disaster_number</th>\n",
       "      <th>state</th>\n",
       "      <th>declaration_type</th>\n",
       "      <th>declaration_date</th>\n",
       "      <th>fy_declared</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>declaration_title</th>\n",
       "      <th>incident_begin_date</th>\n",
       "      <th>incident_end_date</th>\n",
       "      <th>disaster_closeout_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR-1-GA</td>\n",
       "      <td>1</td>\n",
       "      <td>GA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1954-06-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR-2-TX</td>\n",
       "      <td>2</td>\n",
       "      <td>TX</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado &amp; Heavy Rainfall</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1958-01-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR-3-LA</td>\n",
       "      <td>3</td>\n",
       "      <td>LA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1960-02-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR-4-MI</td>\n",
       "      <td>4</td>\n",
       "      <td>MI</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1956-02-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR-5-MT</td>\n",
       "      <td>5</td>\n",
       "      <td>MT</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Floods</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1955-12-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fema_declaration_string  disaster_number state declaration_type  \\\n",
       "0                 DR-1-GA                1    GA               DR   \n",
       "1                 DR-2-TX                2    TX               DR   \n",
       "2                 DR-3-LA                3    LA               DR   \n",
       "3                 DR-4-MI                4    MI               DR   \n",
       "4                 DR-5-MT                5    MT               DR   \n",
       "\n",
       "       declaration_date  fy_declared incident_type         declaration_title  \\\n",
       "0  1953-05-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "1  1953-05-15T00:00:00Z         1953       Tornado  Tornado & Heavy Rainfall   \n",
       "2  1953-05-29T00:00:00Z         1953         Flood                     Flood   \n",
       "3  1953-06-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "4  1953-06-06T00:00:00Z         1953         Flood                    Floods   \n",
       "\n",
       "    incident_begin_date     incident_end_date disaster_closeout_date  \n",
       "0  1953-05-02T00:00:00Z  1953-05-02T00:00:00Z   1954-06-01T00:00:00Z  \n",
       "1  1953-05-15T00:00:00Z  1953-05-15T00:00:00Z   1958-01-01T00:00:00Z  \n",
       "2  1953-05-29T00:00:00Z  1953-05-29T00:00:00Z   1960-02-01T00:00:00Z  \n",
       "3  1953-06-02T00:00:00Z  1953-06-02T00:00:00Z   1956-02-01T00:00:00Z  \n",
       "4  1953-06-06T00:00:00Z  1953-06-06T00:00:00Z   1955-12-01T00:00:00Z  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"us_disaster_declarations.csv\")\n",
    "df.head()\n",
    "\n",
    "# Removing types of disaster unrelated to climate change:\n",
    "unrelated_types = ['Fishing Losses','Other', 'Volcano', 'Toxic Substances', 'Dam/Levee Break','Human Cause', 'Terrorist', 'Chemical', 'Biological']\n",
    "\n",
    "u_rows = []\n",
    "# For loop will loop through each row and keep track with a list which rows are unrelated\n",
    "for index, row in df.iterrows():\n",
    "    if row.incident_type  in unrelated_types:\n",
    "        u_rows.append(index)\n",
    "\n",
    "#Here are the names of columns in the original table, that we do not need\n",
    "garbage = ['designated_area','hash','place_code','declaration_request_number','last_refresh','id','ih_program_declared','ia_program_declared','pa_program_declared','hm_program_declared','fips']\n",
    "\n",
    "# Dropping rows of unrelated types\n",
    "df = df.drop(u_rows)\n",
    "df = df.drop(garbage, 1)\n",
    "\n",
    "# This prints out the head to show what we have done\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2798b",
   "metadata": {},
   "source": [
    "## Removing Duplicate Disasters\n",
    "So one more thing we have to do when cleaning the data set up is removing duplicate disasters. This is important because one hurricane/storm/etc ,for instance, can impact multiple states and counties, and thus be added multiple times to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304d07c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fema_declaration_string</th>\n",
       "      <th>disaster_number</th>\n",
       "      <th>state</th>\n",
       "      <th>declaration_type</th>\n",
       "      <th>declaration_date</th>\n",
       "      <th>fy_declared</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>declaration_title</th>\n",
       "      <th>incident_begin_date</th>\n",
       "      <th>incident_end_date</th>\n",
       "      <th>disaster_closeout_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR-1-GA</td>\n",
       "      <td>1</td>\n",
       "      <td>GA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1953-05-02T00:00:00Z</td>\n",
       "      <td>1954-06-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR-2-TX</td>\n",
       "      <td>2</td>\n",
       "      <td>TX</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado &amp; Heavy Rainfall</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1953-05-15T00:00:00Z</td>\n",
       "      <td>1958-01-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR-3-LA</td>\n",
       "      <td>3</td>\n",
       "      <td>LA</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1953-05-29T00:00:00Z</td>\n",
       "      <td>1960-02-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR-4-MI</td>\n",
       "      <td>4</td>\n",
       "      <td>MI</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1953-06-02T00:00:00Z</td>\n",
       "      <td>1956-02-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR-5-MT</td>\n",
       "      <td>5</td>\n",
       "      <td>MT</td>\n",
       "      <td>DR</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Floods</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1953-06-06T00:00:00Z</td>\n",
       "      <td>1955-12-01T00:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fema_declaration_string  disaster_number state declaration_type  \\\n",
       "0                 DR-1-GA                1    GA               DR   \n",
       "1                 DR-2-TX                2    TX               DR   \n",
       "2                 DR-3-LA                3    LA               DR   \n",
       "3                 DR-4-MI                4    MI               DR   \n",
       "4                 DR-5-MT                5    MT               DR   \n",
       "\n",
       "       declaration_date  fy_declared incident_type         declaration_title  \\\n",
       "0  1953-05-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "1  1953-05-15T00:00:00Z         1953       Tornado  Tornado & Heavy Rainfall   \n",
       "2  1953-05-29T00:00:00Z         1953         Flood                     Flood   \n",
       "3  1953-06-02T00:00:00Z         1953       Tornado                   Tornado   \n",
       "4  1953-06-06T00:00:00Z         1953         Flood                    Floods   \n",
       "\n",
       "    incident_begin_date     incident_end_date disaster_closeout_date  \n",
       "0  1953-05-02T00:00:00Z  1953-05-02T00:00:00Z   1954-06-01T00:00:00Z  \n",
       "1  1953-05-15T00:00:00Z  1953-05-15T00:00:00Z   1958-01-01T00:00:00Z  \n",
       "2  1953-05-29T00:00:00Z  1953-05-29T00:00:00Z   1960-02-01T00:00:00Z  \n",
       "3  1953-06-02T00:00:00Z  1953-06-02T00:00:00Z   1956-02-01T00:00:00Z  \n",
       "4  1953-06-06T00:00:00Z  1953-06-06T00:00:00Z   1955-12-01T00:00:00Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This initial drop, will remove any rows that are identical\n",
    "disaster_nums = df['disaster_number'].drop_duplicates()\n",
    "\n",
    "#Here what we do is we go through the dataframe and add the index's of any repeated disaster\n",
    "num = 0\n",
    "disaster_nums = disaster_nums.to_list()\n",
    "repeated_disasters = []\n",
    "\n",
    "for index, row in df.iterrows(): #This goes through all the rows of a dataframe\n",
    "    if num > len(disaster_nums) - 1:\n",
    "        repeated_disasters.append(index) \n",
    "    elif row.disaster_number != disaster_nums[num]:\n",
    "        repeated_disasters.append(index)\n",
    "    else:\n",
    "        num += 1\n",
    "\n",
    "df = df.drop(repeated_disasters) #This removes all the repeated dataframe indexes that were appended above\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a9eb8",
   "metadata": {},
   "source": [
    "## Mapping the Natural Disaster across the US in an interactive map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28073cb2",
   "metadata": {},
   "source": [
    "A cool thing we can now do with this Data set is Visualize it on an Interactive Map! However, in order to map these disasters on an interactive map , we need to know the longitude and latitude of the locations. However, we are NOT given this, so what we can do is go onto a website like \n",
    "\n",
    "https://www.latlong.net/category/states-236-14.html\n",
    "\n",
    "To ensure that this website doesn't go down or reject our query request, I have copied the html file onto my personal github:\n",
    "\n",
    "https://rvark2002.github.io/Stateswithlatlong.html\n",
    "\n",
    "We will use this to grab our State latitude and longitude and pull the data from a table on there directly. The table on that site contains the state name and the longitude and latitude of the middle of the state. Though this may not be 100% a pinpoint location of the storm, there is not much of a better option without having a proper database which stores latitude and longitude of counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554bee5",
   "metadata": {},
   "source": [
    "## Pulling the State data\n",
    "\n",
    "Here we are going to send a request to the website with latitudes and longitudes (my webpage). We are then going to find the table that is used to store the information. However, we need to do a little bit of html cleaning in order to grab the appropriate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306abd27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2bb72b1af6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#This parses the html that was grabebd from the request above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#This searches for the a table inside the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "r = requests.get(\"https://rvark2002.github.io/Stateswithlatlong.html\") #Sends a request to the website\n",
    "\n",
    "root = BeautifulSoup( r.content, 'html.parser') #This parses the html that was grabebd from the request above\n",
    "\n",
    "data = root.find(\"body\").find(\"main\").findAll(\"table\") #This searches for the a table inside the dataframe\n",
    "\n",
    "data = str(data)\n",
    "data = data[2500:] #We do this so we can pinpoint the exact table we need. 2500 was obtained through trial and error\n",
    "\n",
    "state_table = pd.read_html(str(data)) #This converts the html table into a data frame with State, Latitude, Longitude\n",
    "\n",
    "print(state_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121efcdd",
   "metadata": {},
   "source": [
    "## Modifying the table\n",
    "\n",
    "Unfortunately for us, the Location in the table above is not in the state abbreviation format that we want so our next task is to fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afed48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import requests\n",
    "import datetime\n",
    "import folium\n",
    "\n",
    "r = requests.get(\"https://rvark2002.github.io/Stateswithlatlong.html\")\n",
    "\n",
    "root = BeautifulSoup( r.content, 'html.parser')\n",
    "\n",
    "data = root.find(\"body\").find(\"main\").findAll(\"table\")\n",
    "\n",
    "data = str(data)\n",
    "data = data[2500:]\n",
    "\n",
    "state_table = pd.read_html(str(data))\n",
    "\n",
    "state_df = state_table[0] \n",
    "state_to_abbrev = { #Here is a dictionary of all of the US States and their respective State Abbreviation\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "}\n",
    "\n",
    "#This loop here will go through the state data frame and replace the state name with the State abbreviation\n",
    "for i, row in state_df.iterrows():\n",
    "    s = str(state_df.at[i,'Place Name'])   \n",
    "    s = s[0:s.index(\",\")];\n",
    "    state_df.at[i,'Place Name'] = state_to_abbrev[s]\n",
    "    \n",
    "print(state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99617a9",
   "metadata": {},
   "source": [
    "## Plotting the points (2017-2020)!\n",
    "\n",
    "Great now that we have redid the table so that its in state abbreviation form, lets plot our points! \n",
    "\n",
    "We are going to use foliums map tool to create this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec11f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #We are going to plot all of the Natural Disasters from the years 2017-2020. We chose this 4 year \n",
    "# #time frame because if we plotted a large data set, the code would lag due to the amount of data.\n",
    "\n",
    "# MAP_YEAR = 2017 #Change the value here if you want to see a different range from X to 2021\n",
    "# #Dictionary of all weather types and color code. This will help us to differentiate the different disaster types\n",
    "# weather_to_color = {\n",
    "#     \"Coastal Storm\": 'darkblue',\n",
    "#     \"Drought\": 'orange',\n",
    "#     'Earthquake': 'green',\n",
    "#     'Fire': 'red',\n",
    "#     'Flood': 'purple',\n",
    "#     'Freezing': 'darkpurple',\n",
    "#     'Hurricane': 'pink',\n",
    "#     'Mud/Landslide': 'beige',\n",
    "#     'Severe Ice Storm': 'blue',\n",
    "#     'Severe Storm(s)': 'lightgray',\n",
    "#     'Snow': 'white',\n",
    "#     'Tornado': 'lightred',\n",
    "#     'Tsunami': 'cadetblue',\n",
    "#     'Typhoon': 'lightblue',\n",
    "    \n",
    "# }\n",
    "\n",
    "# #map_osm = folium.Map(location=[39.29, -76.61], zoom_start=5) #This is simply our initial map view/zoom\n",
    "\n",
    "# # for index,row in df.iterrows():\n",
    "# #         if df.at[index,'fy_declared'] > MAP_YEAR: \n",
    "            \n",
    "# #             #This grabs the color that will be used when plotting. This is based on the dictionary above\n",
    "# #             color = weather_to_color[df.at[index,'incident_type']]\n",
    "            \n",
    "# #             #This just grabs the state name for convenience\n",
    "# #             state = df.at[index,'state']\n",
    "            \n",
    "# #             #This description above will be what is shown once it is clicked upon\n",
    "# #             desc = 'Incident: '+df.at[index,'incident_type']+'\\nState: '+state+'\\nDate: '+df.at[index,'declaration_date']+'\\nFema Declaration '+df.at[index,'fema_declaration_string'] \n",
    "            \n",
    "            \n",
    "# #             lat = \"15\" #random value - changed anyway\n",
    "# #             long = \"145\" #random value - changed anyway\n",
    "            \n",
    "# #             randomval1 = random.uniform(-1.25,1.25)\n",
    "# #             randomval2 = random.uniform(-1.25,1.25)\n",
    "# #             #these two random values are to prevent clutter since values will all start at middle of state\n",
    "# #             #What we will do is add the latitude and longitude randomly by this value (x,y)\n",
    "        \n",
    "# #             for j,r in state_df.iterrows():\n",
    "# #                 if state_df.at[j,'Place Name'] == state:\n",
    "# #                     lat = str(float(state_df.at[j,'Latitude']) + randomval1)\n",
    "# #                     long = str(float(state_df.at[j,'Longitude']) + randomval2)\n",
    "# #             folium.Marker(\n",
    "# #             location=[lat,long],\n",
    "# #             popup=str(desc),\n",
    "# #             icon=folium.Icon(color=color),\n",
    "# #             ).add_to(map_osm)\n",
    "       \n",
    "\n",
    "# #map_osm     #This command will run the map\n",
    "\n",
    "# #Unfortunately Github cannot render folium maps so I had to disable it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a02ed",
   "metadata": {},
   "source": [
    "## Analysis of Visualization\n",
    "\n",
    "Now that we can see the map above, here are somethings one can analyze just by looking at it\n",
    "\n",
    "1) Fires are most common on west coast\n",
    "\n",
    "2) Hurricanes are very common on east coast, especially south east\n",
    "\n",
    "3) Flooding appears most along states that border a lake or a river \n",
    "\n",
    "Alot more can be analyzed by looking at maps like this. One can even look at different time periods or single out a disaster. \n",
    "\n",
    "Lets look at a map of Severe Ice Storms in the US over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2f2b5",
   "metadata": {},
   "source": [
    "## Case Study: Severe Ice Storms in US Over Time (1953-2020)\n",
    "\n",
    "Here we are going to look at Severe Ice Storms from 1953-2020. This code will look quite similar to what we did minus a couple parameters. The reason we should look at Ice Storms is because in theory, they should be the most temperature dependent feature, as ice only forms in freezing temperatures. This will play a bigger role below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAP_YEAR = 1953 #Starting year is 1953\n",
    "# map_osm = folium.Map(location=[39.29, -76.61], zoom_start=4) #This is simply our initial map view/zoom\n",
    "\n",
    "\n",
    "# for index,row in df.iterrows():\n",
    "#         if df.at[index,'fy_declared'] > MAP_YEAR: \n",
    "#             if df.at[index,'incident_type'] == 'Severe Ice Storm': #This check will ensure only severe ice storms are plotted\n",
    "            \n",
    "#                 #This grabs the color that will be used when plotting\n",
    "#                 color = 'blue'\n",
    "            \n",
    "#                 #This just grabs the state name for convenience\n",
    "#                 state = df.at[index,'state']\n",
    "            \n",
    "#                 #This description above will be what is shown once it is clicked upon\n",
    "#                 desc = 'Incident: '+df.at[index,'incident_type']+'\\nState: '+state+'\\nDate: '+df.at[index,'declaration_date']+'\\nFema Declaration '+df.at[index,'fema_declaration_string'] \n",
    "            \n",
    "#                 lat = \"15\" #random value - changed anyway\n",
    "#                 long = \"145\" #random value - changed anyway\n",
    "            \n",
    "#                 randomval1 = random.uniform(-1.25,1.25)\n",
    "#                 randomval2 = random.uniform(-1.25,1.25)\n",
    "#                 #these two random values are to prevent clutter since values will all start at middle of state\n",
    "#                 #What we will do is add the latitude and longitude randomly by this value (x,y)\n",
    "        \n",
    "#                 for j,r in state_df.iterrows():\n",
    "#                     if state_df.at[j,'Place Name'] == state:\n",
    "#                         lat = str(float(state_df.at[j,'Latitude']) + randomval1)\n",
    "#                         long = str(float(state_df.at[j,'Longitude']) + randomval2)\n",
    "#                 folium.Marker(\n",
    "#                 location=[lat,long],\n",
    "#                 popup=str(desc),\n",
    "#                 icon=folium.Icon(color=color),\n",
    "#                 ).add_to(map_osm)\n",
    "       \n",
    "\n",
    "# map_osm     #This command will run the map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84623bf",
   "metadata": {},
   "source": [
    "From looking at the map above we can notice a few things.\n",
    "1) Severe Ice Storms are almost non existant on the West Coast of America/Alaska\n",
    "\n",
    "2) The incidents a primarily declared in middle america around Kansas/Oklahoma/Missouri\n",
    "\n",
    "3) An interesting observation is that typical cold places like Alaska have no declarations there\n",
    "\n",
    "This same idea can be applied to all of the other types of natural disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c17f43",
   "metadata": {},
   "source": [
    "## Which states are the most prone to Natural Disasters? \n",
    "\n",
    "Lets see if any states are more disaster prone than others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_disaster_dict = {} #This creates a dictionary in which we can store the number of disasters in that state\n",
    "\n",
    "for i,row in df.iterrows(): #This will go through the data frame and tally the number of disasters per state\n",
    "    state = df.at[i,'state']\n",
    "    if state in state_disaster_dict:\n",
    "        state_disaster_dict[state] = state_disaster_dict[state] + 1\n",
    "    else:\n",
    "        state_disaster_dict[state] = 1\n",
    "        \n",
    "#print(state_disaster_dict)\n",
    "\n",
    "#What we should do now is combine all the states that have less than 100 disasters into an other category\n",
    "\n",
    "cleaned_dict = {} #remove all ones less than 100 and put in other category\n",
    "other = 0\n",
    "for elem in state_disaster_dict.keys():\n",
    "    if state_disaster_dict[elem] <= 100:\n",
    "        other += state_disaster_dict[elem]\n",
    "    else:\n",
    "        cleaned_dict[elem] = state_disaster_dict[elem]\n",
    "\n",
    "cleaned_dict['other'] = other       \n",
    "sdf = pd.DataFrame(cleaned_dict.items())\n",
    "sdf = sdf.sort_values(by=[1],ascending=False)\n",
    "plot = sdf.plot.pie(y=1,labels = sdf[0],figsize=(8, 8))\n",
    "plot\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f680a",
   "metadata": {},
   "source": [
    "Looking at the totals above, we can tell that Texas has had the most disasters from 1953-2021, however lets take a closer look by analyzing this based on time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65c56a",
   "metadata": {},
   "source": [
    "## Which states are the most prone to Natural Disasters? (1953-1970)  (1971-1988) (1988-2005) (2006-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28518d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================\n",
    "#1953 - 1970\n",
    "#====================\n",
    "state_disaster_dict = {} #This creates a dictionary in which we can store the number of disasters in that state\n",
    "max = 1970 #This is our min and max value that we change as it goes on\n",
    "min = 1953\n",
    "val = 7\n",
    "for i,row in df.iterrows(): #This will go through the data frame and tally the number of disasters per state\n",
    "    state = df.at[i,'state']\n",
    "    if df.at[i,'fy_declared'] >= min and df.at[i,'fy_declared'] <= max:\n",
    "        if state in state_disaster_dict:\n",
    "            state_disaster_dict[state] = state_disaster_dict[state] + 1\n",
    "        else:\n",
    "            state_disaster_dict[state] = 1\n",
    "\n",
    "cleaned_dict = {} \n",
    "other = 0\n",
    "for elem in state_disaster_dict.keys():\n",
    "    if state_disaster_dict[elem] <= val:\n",
    "        other += state_disaster_dict[elem]\n",
    "    else:\n",
    "        cleaned_dict[elem] = state_disaster_dict[elem]\n",
    "\n",
    "cleaned_dict['other'] = other       \n",
    "sdf = pd.DataFrame(cleaned_dict.items())\n",
    "sdf = sdf.sort_values(by=[1],ascending=False)\n",
    "plot = sdf.plot.pie(y=1,labels = sdf[0],figsize=(8, 8))\n",
    "plot\n",
    "print('1953 - 1970')\n",
    "print(sdf)\n",
    "\n",
    "#====================\n",
    "#1971 - 1988\n",
    "#====================\n",
    "state_disaster_dict = {} #This creates a dictionary in which we can store the number of disasters in that state\n",
    "max = 1988\n",
    "min = 1971\n",
    "val = 20\n",
    "for i,row in df.iterrows(): #This will go through the data frame and tally the number of disasters per state\n",
    "    state = df.at[i,'state']\n",
    "    if df.at[i,'fy_declared'] >= min and df.at[i,'fy_declared'] <= max:\n",
    "        if state in state_disaster_dict:\n",
    "            state_disaster_dict[state] = state_disaster_dict[state] + 1\n",
    "        else:\n",
    "            state_disaster_dict[state] = 1\n",
    "cleaned_dict = {}\n",
    "other = 0\n",
    "for elem in state_disaster_dict.keys():\n",
    "    if state_disaster_dict[elem] <= val:\n",
    "        other += state_disaster_dict[elem]\n",
    "    else:\n",
    "        cleaned_dict[elem] = state_disaster_dict[elem]\n",
    "cleaned_dict['other'] = other       \n",
    "sdf = pd.DataFrame(cleaned_dict.items())\n",
    "sdf = sdf.sort_values(by=[1],ascending=False)\n",
    "plot = sdf.plot.pie(y=1,labels = sdf[0],figsize=(8, 8))\n",
    "plot\n",
    "print('1971 - 1988')\n",
    "print(sdf)\n",
    "#====================\n",
    "#1989 - 2005\n",
    "#====================\n",
    "state_disaster_dict = {}\n",
    "max = 2005\n",
    "min = 1989\n",
    "val = 30\n",
    "for i,row in df.iterrows(): #This will go through the data frame and tally the number of disasters per state\n",
    "    state = df.at[i,'state']\n",
    "    if df.at[i,'fy_declared'] >= min and df.at[i,'fy_declared'] <= max:\n",
    "        if state in state_disaster_dict:\n",
    "            state_disaster_dict[state] = state_disaster_dict[state] + 1\n",
    "        else:\n",
    "            state_disaster_dict[state] = 1\n",
    "cleaned_dict = {} \n",
    "other = 0\n",
    "for elem in state_disaster_dict.keys():\n",
    "    if state_disaster_dict[elem] <= val:\n",
    "        other += state_disaster_dict[elem]\n",
    "    else:\n",
    "        cleaned_dict[elem] = state_disaster_dict[elem]\n",
    "\n",
    "cleaned_dict['other'] = other       \n",
    "sdf = pd.DataFrame(cleaned_dict.items())\n",
    "sdf = sdf.sort_values(by=[1],ascending=False)\n",
    "plot = sdf.plot.pie(y=1,labels = sdf[0],figsize=(8, 8))\n",
    "plot\n",
    "print('1989 - 2005')\n",
    "print(sdf)\n",
    "\n",
    "#====================\n",
    "#2006 - 2021\n",
    "#====================\n",
    "state_disaster_dict = {}\n",
    "max = 2021\n",
    "min = 2006\n",
    "val = 40\n",
    "\n",
    "for i,row in df.iterrows(): #This will go through the data frame and tally the number of disasters per state\n",
    "    state = df.at[i,'state']\n",
    "    if df.at[i,'fy_declared'] >= min and df.at[i,'fy_declared'] <= max:\n",
    "        if state in state_disaster_dict:\n",
    "            state_disaster_dict[state] = state_disaster_dict[state] + 1\n",
    "        else:\n",
    "            state_disaster_dict[state] = 1\n",
    "\n",
    "cleaned_dict = {} \n",
    "other = 0\n",
    "for elem in state_disaster_dict.keys():\n",
    "    if state_disaster_dict[elem] <= val:\n",
    "        other += state_disaster_dict[elem]\n",
    "    else:\n",
    "        cleaned_dict[elem] = state_disaster_dict[elem]\n",
    "\n",
    "cleaned_dict['other'] = other       \n",
    "sdf = pd.DataFrame(cleaned_dict.items())\n",
    "sdf = sdf.sort_values(by=[1],ascending=False)\n",
    "plot = sdf.plot.pie(y=1,labels = sdf[0],figsize=(8, 8))\n",
    "plot\n",
    "print('2006 - 2021')\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601401d6",
   "metadata": {},
   "source": [
    "Pie Charts Analyzed - \n",
    "\n",
    "When comparing the 5 pie charts (including total one above) next to each other, we can see that Texas and California were both consistently the two states with the most disasters. However, Disasters seem to be increasing dramatically in recent years. Take Texas for instance, in the 1953-1971 it received less than 20 disasters, while 156 in 2006-2021. Texas is not the only state seeing this trend. Lets look at this closer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bac51d",
   "metadata": {},
   "source": [
    "## Natural Disaster Totals Per Year\n",
    "\n",
    "Here lets now look at Natural Disasters over time now. Lets start by obtaining the Natural Disaster totals per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b631da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting number of natural disasters per year: \n",
    "\n",
    "pdf = pd.DataFrame(df,columns=['fy_declared','incident_type'])\n",
    "\n",
    "pdf = pdf.rename(columns={'fy_declared':'year','incident_type':'type'})\n",
    "\n",
    "pdf['dummy'] = np.ones(len(df)) # Changed this to len(df) for it to be more dynamic\n",
    "\n",
    "pdf = pdf.groupby(by=['year','type']).count().unstack() #This adds up the disasters per year, most important line!\n",
    "\n",
    "#This part is so that we can get rid of the outer part of the dataframe,\n",
    "#and filling all empty spots with 0\n",
    "pdf = pdf['dummy']\n",
    "pdf = pdf.fillna(0) \n",
    "\n",
    "#This is to add the sum of all the rows together\n",
    "pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "\n",
    "if 'totals' in pdf.columns:\n",
    "    pdf = pdf.drop('totals',1)\n",
    "totals = []\n",
    "for index, row in pdf.iterrows():\n",
    "    t = 0\n",
    "    for i in row:\n",
    "        t += i\n",
    "    totals.append(int(t))\n",
    "    \n",
    "pdf['totals'] = totals\n",
    "\n",
    "pdf.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ad273",
   "metadata": {},
   "source": [
    "## Plotting Total Number of Natural Disasters over Time\n",
    "\n",
    "Here we are going to plot the total number of Natural Disasters over Time. This can be easily be done by using the totals column that we made in the previous bit of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1953,2023)\n",
    "pdf['year'] = years\n",
    "\n",
    "#Removed due to incomplete data\n",
    "pdf.drop(pdf[pdf['year'] ==2021].index, inplace = True)\n",
    "pdf.plot(x = 'year', y = 'totals', kind='scatter',title= 'Total Number of Natural Disasters per Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e863ae1",
   "metadata": {},
   "source": [
    "## Linear Regression of Total Number of Natural Disasters over Time\n",
    "\n",
    "To better understand the scatterplot above, lets run a linear regression model on it and plot the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to make 3 new imports to help us run some regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x = pdf['year'].values.reshape(-1, 1)\n",
    "y = pdf['totals'].values.reshape(-1, 1)\n",
    "reg = LinearRegression()\n",
    "reg.fit(x, y)\n",
    "y_pred = reg.predict(x)\n",
    "print (str(reg.coef_[0][0])+ 'x '+ str(reg.intercept_[0])) #This prints the linear equation out\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, y_pred, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6267a2",
   "metadata": {},
   "source": [
    "As you can see above, the amount of Natural Disasters in the US is increasing over time. \n",
    "\n",
    "What could be causing this increase? We will look into it more below. But first lets look at these disasters individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602be13",
   "metadata": {},
   "source": [
    "## Plotting a Polynomial regression of the total number of each Disaster Type by Year \n",
    "\n",
    "For the next part, instead of running a linear regression which may not be helpful on these scatterplots, lets run a polynomial regression to the 5th degree. We should also run it on each of the different disasters to study/analyze independent trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae51d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop will go through each column. However since year and totals is also a column in our dataframe, we\n",
    "# should just ignore them.\n",
    "for elem in pdf:\n",
    "    if elem != 'totals' and elem != 'year':\n",
    "        x = pdf['year'].values.reshape(-1, 1)\n",
    "        y = pdf[elem].values.reshape(-1, 1)\n",
    "        degree = 5 #This gets our degree\n",
    "        reg=make_pipeline(PolynomialFeatures(degree),LinearRegression()) #This runs the polynomial regression\n",
    "        reg.fit(x, y)\n",
    "        y_pred = reg.predict(x)\n",
    "        plt.title(elem)\n",
    "        plt.scatter(x,y)\n",
    "        plt.plot(x, y_pred, color='red')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a5f0f",
   "metadata": {},
   "source": [
    "## Analysis of Graphs\n",
    "\n",
    "So here we have our graphs of all the Disasters over time.\n",
    "\n",
    "Lets look at this independently:\n",
    "\n",
    "1) Coastal Storm - Increasing over time\n",
    "\n",
    "2) Drought - Decreasing over time\n",
    "\n",
    "3) Earthquake - Increasing over time\n",
    "\n",
    "4) Fire - Increasing over time, however seems to be leveling off?\n",
    "\n",
    "5) Flood - Increased, then decreased, then appears to be once again on the rise\n",
    "\n",
    "6) Freezing - Increased, but then began to decrease since the 90's\n",
    "\n",
    "7) Hurricane - Increasing over time\n",
    "\n",
    "8) Mud/landslide - Increasing in recent years\n",
    "\n",
    "9) Severe Ice Storms - Was increasing for a period, with some strong storms pulling the data up, however over all trend seems to be decreasing now.\n",
    "\n",
    "10) Severe Storms - was increasing quite a bit, was also pulled up by some high frequencies of storms in the 2000's however seems to be decreasing in modern years\n",
    "\n",
    "11) Snow - was increasing until it hit a sharp decline in the mid 2000's\n",
    "\n",
    "12) Tornado's have been decreasing since the 80's\n",
    "\n",
    "13) Tsunami graph- only has 1 data point, so no real analysis can be drawn\n",
    "\n",
    "14) Typhoon - Was increasing until the mid 2000's, but is now decreasing\n",
    "\n",
    "When looking at these graphs one can raise many questions about their trends. For instance the Freezing/Snow related storms seem to all be on the decline, while coastal storms, hurricane like storms seem to be on the incline. \n",
    "\n",
    "What factors could be causing this change? One possibility could be rising temperatures due to climate change. We will analyze that below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e8978",
   "metadata": {},
   "source": [
    "\n",
    "## Correlation Between Temperature and Natural Disaster\n",
    "\n",
    "We have all heard scientists on the news warning us on how climate change will increase the frequency and severuty of natural disasters. Therefore, for this next part we will be observing how the number of natural disasters and change in temperature over time correlate with each other. \n",
    "\n",
    "Observing the correlation between temperature and number of natural disasters might be more productive than solely connecting number of natural disasters with time. Time is a variable which can only increase, therefore, making time not suitable of a variable for the number of natural disasters to depend on. Temperature on the other hand cannot solely increase, as well as having scientific background in regards to global warming, which makes it more significant variable which can affects the number of natural disasters per year.\n",
    "\n",
    "Now, to look at the correlation between temperature and number of natural disasters.\n",
    "\n",
    "We will be using the average temperature by year dataset provided by the \"NOAA National Centers for Environmental information\" from: https://www.ncdc.noaa.gov/cag/national/time-series/110/tavg/12/1/1953-2021?base_prd=true&begbaseyear=1901&endbaseyear=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_temp = pd.read_csv(\"110-tavg-12-1-1953-2021.csv\")\n",
    "us_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1453d0",
   "metadata": {},
   "source": [
    "We will be adding the column with temperatures by year to our previously made dataset: pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be adding the column with temperatures by year to our previously made dataset: pdf\n",
    "temps = us_temp['Value'].tolist()\n",
    "pdf['temps'] = temps # adding new column with temperatures to pdf\n",
    "x = pdf['year'].values.reshape(-1, 1)\n",
    "y = pdf['temps'].values.reshape(-1, 1)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d1c41",
   "metadata": {},
   "source": [
    "We can observe from the plot that the temperatures seem to be continually increasing since the mid 1960s after an initial drop. The temperature has an increasing nature similar to that of number of natural disasters. Now let's quantify the correlation between the fluctuations in temperatre and number of natural disasters by finding the correlation coeficient between the two. There is a function included in pandas which can calculate it.\n",
    "\n",
    "Reminder that a positive correlation would indicate that as one variable increases, the other does as well, and a negative correlation as one increases, the other decreases.\n",
    "\n",
    "## Correlation between each natural disaster and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using a pandas function which can calculate the correlation coeficient between two columns\n",
    "column_1 = pdf['totals']\n",
    "column_2 = pdf[\"temps\"]\n",
    "correlation = column_1.corr(column_2)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dc0ce",
   "metadata": {},
   "source": [
    "We can see that there exists a positive correlation between the total amount of natural disasters and average temperature by year. \n",
    "\n",
    "However, as we can observe from the  previously plotted total number of natural disasters per year by disaster, not every disaster is increasing as time goes by. Therefore, we are now going to calculate the correlation coeficient between the number of natural disasters per year according to each natural disaster type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two lists will serve to populate a dataframe with the disaster type and te given correlation coeficient\n",
    "disasters = []\n",
    "correlations = []\n",
    "for disaster in pdf:\n",
    "    if disaster not in ['totals', 'year', 'temps']: # ignoring these columns\n",
    "        column_1 = pdf[disaster] \n",
    "        column_2 = pdf[\"temps\"]\n",
    "        correlation = column_1.corr(column_2)\n",
    "        disasters.append(disaster)\n",
    "        correlations.append(correlation)\n",
    "\n",
    "d = {'disaster': disasters,'correlation':correlations }  \n",
    "cor_df = pd.DataFrame.from_dict(d)      \n",
    "cor_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "barp = cor_df.plot.bar('disaster', 'correlation',title='Correlation Between Temperature and Disaster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18832b80",
   "metadata": {},
   "source": [
    "## Conclusion from the relationship between temperature and number of natural disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898b936",
   "metadata": {},
   "source": [
    "We can now see each natural disaster and it's correlation with temperature.\n",
    "\n",
    "1) Coastal Storm - Medium Positive correlation\n",
    "\n",
    "2) Drought - Negative correlation\n",
    "\n",
    "3) Earthquake - Negligible  correlation\n",
    "\n",
    "4) Fire - The Strongest Positive correlation, which intuitively makes sense, hotter temperature would mean more\n",
    "\n",
    "5) Flood - Negative correlation\n",
    "\n",
    "6) Freezing - Low Negative correlation\n",
    "\n",
    "7) Hurricane - Strong Positive correlation\n",
    "\n",
    "8) Mud/landslide - Positive correlation\n",
    "\n",
    "9) Severe Ice Storms - Low Positive correlation\n",
    "\n",
    "10) Severe Storms - Strong Positive correlation\n",
    "\n",
    "11) Snow - Low Positive correlation\n",
    "\n",
    "12) Tornado - Low Negative correlation\n",
    "\n",
    "13) Tsunami - No correlation\n",
    "\n",
    "14) Typhoon - Positive correlation\n",
    "\n",
    "Looking at the correlations, we can see that most Natural Disasters seem to be correlated positively with the increase of Temperature over time. However, CORRELATION DOES NOT EQUAL CAUSATION, so we cannot conclude that the increase in temperature is causing Natural Disasters to be more frequent but we can say there appears to be a correlation between the two. \n",
    "\n",
    "Bellow we can visualize the correlation between natural disasters and temperatures. We will be graphing the amount of natural disasters per temperature. Steeper lines indicate stronger correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for disaster in pdf:\n",
    "    if disaster not in ['totals', 'year', 'temps']: # ignoring these columns\n",
    "        fires = pdf[disaster].tolist()\n",
    "        temperatures = pdf['temps'].tolist()\n",
    "        d = {'fire': fires, 'temp': temperatures}\n",
    "        fire_df = pd.DataFrame.from_dict(d)   \n",
    "        x = fire_df['temp'].values.reshape(-1, 1)\n",
    "        y = fire_df['fire'].values.reshape(-1, 1)\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(x, y)\n",
    "        y_pred = reg.predict(x)\n",
    "        plt.scatter(x,y)\n",
    "        plt.plot(x, y_pred, color='red')\n",
    "        plt.title(disaster+ 'per temperature in Fahrenheit')\n",
    "        plt.xlabel('Temperature')\n",
    "        plt.ylabel('Number of '+disaster)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfdde9",
   "metadata": {},
   "source": [
    "We can use these linear models to possibly predict how many of each disaster will increase as temperature increases. Temperature might be a better variable for number of natural disasters to depend on. However, as previously mentioned, just because there exists correlation between temperature and number of natural disasters does not mean it is the cause. There are many factors which go into natural disasters. Temperature might seem like one, which is more relevant to some types of disasters over other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66923878",
   "metadata": {},
   "source": [
    "## Limitations of Analysis of Data\n",
    "\n",
    "No data is ever perfect for analysis. Limitations exist in all data collected and here we are going to talk/speculate about some that may exist in our Data set.\n",
    "\n",
    "1) How they registered natural disasters over time - In earlier years, natural disasters may not have been stored as accurately as they were in modern years with new technology. The classifications of natural disasters could have changed over time - would a severe storm in 1953 be at the same strength as a severe storm from 2010?\n",
    "\n",
    "2) Unseen Variables - There are multiple variables at play when it comes to Natural Disasters. Each Natural disaster is quite different and caused by different things. We may see a correlation between one variable and another , but not see a correlation between another two. This does not mean that something like Temperature isnt impacting a certain Natural Disaster.\n",
    "\n",
    "3) Our Data set may not be big enough - Though, Natural Disasters are catostrophic events with long term ramifications, they do not happen as often as we may expect. Therefore our sample size may be too small to come to appropriate conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a346c5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## What we covered in our tutorial:\n",
    "1) Reading in a CSV file\n",
    "\n",
    "2) Cleaning/Tidying a Data Frame\n",
    "\n",
    "3) Scraping Data from a webpage\n",
    "\n",
    "4) Merging Data Frames\n",
    "\n",
    "5) Using folium to create Interactive Maps\n",
    "\n",
    "6) Creating pie charts focused over periods of time\n",
    "\n",
    "7) Creating Scatterplots\n",
    "\n",
    "8) Creating Linear Regression Lines and plotting them\n",
    "\n",
    "9) Creating Polynomial Regression Lines and plotting them\n",
    "\n",
    "10) Finding and Graphing Correlation Coefficients\n",
    "\n",
    "## Conclusion of Data Analysis\n",
    "\n",
    "In our analysis of Natural Disasters we found that certain States seemed to be more prone to certain disasters than others. Including that some Disasters exclusively only happened in certain regions of the US (Fires and West Coast). In making Pie Charts we found that some states like Texas had much more Natural Disasters from 1953-2021 compared to other states. However, upon dividing up the chunk of time into time periods, we found that this was not always the case, and in recent years, there appears to be an uptick of Natural Disasters. When looking at the Totals of Natural Disasters per year through a Linear Regression Model, we could clearly see there was an increase in disasters. Singling out the Disasters, and running Polynomial Regression Models on them showed us that certain Disasters saw an uptick in frequency as time progressed, leading us to believe that there may be something causing this uptick. We then decided to compare the Natural Disasters frequencies to Temperature change in the US. This resulted in us finding a mostly Positive correlation between Natural Disasters and Temperature. After finding the correlation we decided to create linear regressions to predict how the change in average temperature might affect the number for each natural disaster as the world gets warmer. Though we cannot conclude that temperature is a causation factor, we can claim there is a correlation between the two. Natural Disasters, will continue to remain a threat to US society, and according to our models, Natural Disasters appear to be becoming more and more frequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45414437",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "1) https://ourworldindata.org/natural-disasters - for more data\n",
    "\n",
    "2) https://www.thezebra.com/resources/research/natural-disaster-statistics/ - more statistics/data\n",
    "\n",
    "3) https://www.dosomething.org/us/facts/11-facts-about-disasters - fun article about natural disasters\n",
    "\n",
    "4) https://www.statista.com/topics/2155/natural-disasters/ - additional stats\n",
    "\n",
    "5) https://www.ready.gov/kids/disaster-facts - general information about disasters\n",
    "\n",
    "6) https://www.w3schools.com/python/python_ml_polynomial_regression.asp - help on Polynomial and other kinds of Regression models\n",
    "\n",
    "7) https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html - For help on Data Frames and its features\n",
    "\n",
    "8) http://python-visualization.github.io/folium/quickstart.html#Markers - A quick guide to Folium and Map Making/Marking\n",
    "\n",
    "9) https://datatofish.com/plot-dataframe-pandas/ - Great guide to creating different graphs out of Data frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
